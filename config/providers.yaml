# Provider Configuration
providers:
  - name: llama2:latest
    type: llama  # keep it the same, matches the provider class
    enabled: true
    priority: 1
    endpoint: "http://localhost:11434/api/generate"
    model: "llama2:latest"
    timeout: 10
    retry_count: 2
    cost_per_1k_tokens:
      prompt: 0.0
      completion: 0.0
    max_tokens: 2048
    context_size: 4096
  - name: huggingface
    type: huggingface
    enabled: true
    priority: 2
    api_key: ************************** 
    model: "google/flan-t5-base"  # A smaller model that can run on the free tier
    timeout: 15
    retry_count: 2
    cost_per_1k_tokens:
      prompt: 0.0  # Free tier
      completion: 0.0  # Free tier
    max_tokens: 512
    context_size: 1024

  - name: groq
    type: groq
    endpoint: "https://api.groq.com/openai/v1/chat/completions"
    priority: 1
    cost_per_1k_tokens: 0.002
    api_key: "****************************************"
    model: "llama-3.1-8b-instant"
    

# Global settings
settings:
  default_max_tokens: 100
  default_temperature: 0.7
  log_level: INFO
