# ğŸ§  LLM Provider Framework â€“ Local + Cloud AI Powered Backend

This project offers a modular backend framework to integrate and switch between multiple LLM providers including **Ollama (local models)**, **Groq (cloud API)**, and **HuggingFace (cloud API)**. It includes automatic fallback between providers, token counting, retry logic, and detailed logs.

---

## ğŸ“¦ Features

- âœ… Multiple LLM providers: Ollama (local), Groq, HuggingFace
- ğŸ”„ Fallback if one provider fails
- ğŸ” API Key support for secure cloud calls
- ğŸ“Š Token usage tracking
- ğŸ“œ JSON-based prompt sending and response handling
- ğŸ§ª Easily extendable for new providers

---

## ğŸš€ Quick Start

### ğŸ“ Clone the Repository

```bash
git clone https://github.com/your-username/your-llm-project.git
cd your-llm-project
```

### ğŸ§° Create Virtual Environment

```bash
python3 -m venv venv
source venv/bin/activate
```

### ğŸ“¦ Install Dependencies

```bash
pip install -r requirements.txt
```

---

## ğŸ”§ Configuration

Edit or create a `config.yaml` file:

```yaml
providers:
  - name: llama2:latest
    type: ollama
    model: llama2
  - name: groq
    type: groq
    model: llama3-8b-8192
    api_key: YOUR_GROQ_API_KEY
  - name: huggingface
    type: huggingface
    model: google/flan-t5-small
    api_key: YOUR_HUGGINGFACE_API_KEY
```

You can also set API keys via environment variables:

```bash
export GROQ_API_KEY="your-groq-key"
export HF_API_KEY="your-hf-key"
```

---

## âš™ï¸ Running the Server

```bash
python main.py
```

The app will start on:

```
http://127.0.0.1:5000
```

---

## ğŸ§ª Test the API

Use `curl` or Postman to send a test prompt:

```bash
curl -X POST http://127.0.0.1:5000/generate      -H "Content-Type: application/json"      -d '{"prompt": "Explain the theory of relativity", "max_tokens": 100, "temperature": 0.7}'
```

---

## ğŸ¤– Using Local Models via Ollama (llama2, codellama, etc.)

### ğŸ”¹ 1. Install Ollama

Download and install from: [https://ollama.com](https://ollama.com)

### ğŸ”¹ 2. Start Ollama

```bash
ollama serve
```

### ğŸ”¹ 3. Pull a Model (e.g. llama2)

```bash
ollama pull llama2
```

### ğŸ”¹ 4. Test Locally (optional)

```bash
ollama run llama2
```

> â„¹ï¸ Ensure Ollama is running on `http://localhost:11434`
> 
> If you see timeouts, confirm:
> - Ollama is running
> - Model is downloaded
> - No firewall is blocking port `11434`

---

## âš ï¸ Known Issues & Fixes

### ğŸ§¨ Ollama Timeout

```log
Ollama request failed: HTTPConnectionPool(host='localhost', port=11434): Read timed out.
```

**Fix:**

```bash
ollama serve        # Make sure it's running
ollama pull llama2  # Pull the required model
```

---

### ğŸ› Groq: `'float' object has no attribute 'get'`

Fix in `groq_provider.py`:

```python
usage = result.get("usage") or {}
```

This avoids errors when API returns null, float, or invalid structure.

---

## ğŸ”„ Provider Fallback Order

The system automatically tries the next provider if the current one fails:

```text
llama2 (local via Ollama) â†’ groq â†’ huggingface
```

Logs will clearly show:
- Which provider is initialized
- Which one is selected
- Errors/fallbacks (if any)

---

## ğŸ—‚ï¸ Project Structure

```
.
â”œâ”€â”€ main.py                     # Entry point (Flask API)
â”œâ”€â”€ config.yaml                 # Config for all providers
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ provider_manager.py     # Handles fallback and initialization
â”‚   â””â”€â”€ providers/
â”‚       â”œâ”€â”€ llama_provider.py   # Ollama integration
â”‚       â”œâ”€â”€ groq_provider.py    # Groq API integration
â”‚       â””â”€â”€ huggingface_provider.py
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ logger.py               # Logging setup
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âœ… Requirements

- Python 3.9+
- `ollama` installed for local model inference
- API keys for Groq & HuggingFace if cloud generation is needed

---
## OUTPUTS
outputs

(https://github.com/devi-harikaa/MultiLlm_router-assignment-solution/blob/f1c56f7b87c6fe818bcc07db3d8c9beecfe66107/outputs/Screenshot%202025-04-07%20211126.png)
